import datetime
import os
import shutil
import gym
import torch
import torch.nn as nn
from torch.distributions import Normal
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, TensorDataset

import evaluate_updraft_exploiter
from parameters import params_environment, params_triangle_soaring
from subtasks.updraft_exploiter import params_updraft_exploiter
from utils.LSTM_model import ActorCritic
from utils.PPO_data_handling import PPOBuffer, SlidingWindowDataset
import utils.Spinup_core as core

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


class PPO:
    def __init__(self):
        # instantiate parameters
        self._params_rl = params_updraft_exploiter.params_rl()
        self._params_model = params_updraft_exploiter.params_model()

        # instantiate buffer for rollout
        self.buffer = PPOBuffer(self._params_model.DIM_IN, self._params_model.DIM_OUT, self._params_rl.BATCHSIZE,
                                self._params_model.DIM_LSTM, gamma=self._params_rl.GAMMA, lam=self._params_rl.LAMBDA,
                                device=device)

        # instantiate actor-critic model
        self.model = ActorCritic(params_model=self._params_model, params_rl=self._params_rl, device=device).to(device)
        # self.model.actor.load_state_dict(torch.load("decision_maker_actor_final_03-November-2020_15-17.pt"))
        # self.model.critic.load_state_dict(torch.load("decision_maker_critic_final_03-November-2020_15-17.pt"))

        # setup optimizers
        self.pi_optimizer = torch.optim.Adam(self.model.actor.parameters(), lr=self._params_rl.LEARNING_RATE_PI)
        self.vf_optimizer = torch.optim.Adam(self.model.critic.parameters(), lr=self._params_rl.LEARNING_RATE_VF)

    def select_action(self, state, lstm_hidden, validation_mask=False):
        # evaluate decision maker
        action, action_logprob, lstm_hidden = self.model.act(state, lstm_hidden, validation_mask=validation_mask)

        # evaluate critic
        state_value = self.model.critic(state)

        return action, action_logprob, state_value, lstm_hidden

    def update(self):

        # get sampled data
        data = self.buffer.get()
        obs, act, ret, adv, logp, lstm_h_in, lstm_c_in, done \
            = data['obs'], data['act'], data['ret'], data['adv'], data['logp'], \
              data['lstm_h_in'], data['lstm_c_in'], data['done']

        # put batch to sliding-window data_loader
        data_set = SlidingWindowDataset(TensorDataset(obs, act, ret, adv, logp, lstm_h_in, lstm_c_in, done),
                                        self._params_rl.SEQ_LEN)

        # Optimize policy for K epochs:
        for _ in range(self._params_rl.K_EPOCH):
            for start_index in range(0, (data_set.__len__()) + 1,
                                     int(self._params_rl.SEQ_LEN - self._params_rl.OVERLAP)):
                # get sampled sequence/mini-batch
                obs_seq, act_seq, ret_seq, adv_seq, logp_seq, lstm_h_in_seq, lstm_c_in_seq, done_seq = \
                    data_set.__getitem__(start_index)

                if any(done_seq == 1):
                    #  never evaluate sequences that cross episode boundaries
                    done_index = done_seq.nonzero()[0].item()  # index of first done flag in sequence
                    if done_index > (self._params_rl.SEQ_LEN_MIN - 1):
                        obs_seq = obs_seq[slice(0, done_index + 1)]
                        act_seq = act_seq[slice(0, done_index + 1)]
                        ret_seq = ret_seq[slice(0, done_index + 1)]
                        adv_seq = adv_seq[slice(0, done_index + 1)]
                        logp_seq = logp_seq[slice(0, done_index + 1)]
                        lstm_h_in_seq = lstm_h_in_seq[slice(0, done_index + 1)]
                        lstm_c_in_seq = lstm_c_in_seq[slice(0, done_index + 1)]
                    else:
                        break

                # "burn in" lstm hidden state (cf. "R2D2")
                with torch.no_grad():
                    _, (lstm_h_burned_in, lstm_c_burned_in) = \
                        self.model.actor(obs_seq[0:self._params_rl.N_BURNIN, :], (lstm_h_in_seq[0], lstm_c_in_seq[0]))

                # evaluate policy for remainder sampled sequence of states and actions
                logp_eval = self.model.evaluate_actor(obs_seq[self._params_rl.N_BURNIN:, :],
                                                      act_seq[self._params_rl.N_BURNIN:],
                                                      (lstm_h_burned_in, lstm_c_burned_in))

                # ppo ratio
                ratios = torch.exp(logp_eval - logp_seq[self._params_rl.N_BURNIN:])

                # surrogate loss (PPO)
                surr1 = ratios * adv_seq[self._params_rl.N_BURNIN:]
                surr2 = torch.clamp(ratios, 1 - self._params_rl.EPS_CLIP, 1 + self._params_rl.EPS_CLIP) \
                        * adv_seq[self._params_rl.N_BURNIN:]
                loss_pi = -torch.min(surr1, surr2).mean()

                # policy gradient step
                self.pi_optimizer.zero_grad()
                loss_pi.backward()
                self.pi_optimizer.step()

                # value function gradient step
                loss_vf = ((self.model.critic(obs_seq) - ret_seq) ** 2).mean()
                self.vf_optimizer.zero_grad()
                loss_vf.backward()
                self.vf_optimizer.step()


def main():
    # set up training
    env = gym.make('glider3D-v0', agent='updraft_exploiter_uninformed')

    # instantiate agent
    ppo = PPO()

    # load parameters
    _params_rl = params_updraft_exploiter.params_rl()
    _params_agent = params_updraft_exploiter.params_agent()
    _params_logging = params_updraft_exploiter.params_logging()

    # create folder to store data for the experiment running
    experimentID = 1
    dirName = "{}_uninformed_updraft_exploiter_experiment".format(experimentID)
    while os.path.exists(dirName):
        experimentID += 1
        dirName = "{}_".format(experimentID) + dirName.split('_', 1)[1]
    os.mkdir(dirName)
    shutil.copytree(os.getcwd(), os.path.join(dirName, "Sources_unzipped"),
                    ignore=shutil.ignore_patterns('*experiment*', 'archive', 'tests', '.git*', 'rl_ccs_experiments',
                                                  '.idea', '__pycache__', 'README*'))
    os.chdir(dirName)
    shutil.make_archive("Sources", 'zip', "Sources_unzipped")
    shutil.rmtree("Sources_unzipped")
    print("Directory for running experiment no. {} created".format(experimentID))

    # save parameters to file
    parameterFile = open("parameterFile.txt", "w")
    parameterFile.write(
        format(vars(_params_rl)) + "\n" +
        format(vars(params_updraft_exploiter.params_model())) + "\n" +
        format(vars(_params_agent)) + "\n" +
        format(vars(_params_logging)) + "\n\n" +
        format(vars(params_triangle_soaring.params_task())) + "\n\n" +
        format(vars(params_environment.params_sim())) + "\n" +
        format(vars(params_environment.params_glider())) + "\n" +
        format(vars(params_environment.params_physics())) + "\n" +
        format(vars(params_environment.params_wind())))
    parameterFile.close()

    # set random seed
    if _params_rl.SEED:
        print("Random Seed: {}".format(_params_rl.SEED))
        torch.manual_seed(_params_rl.SEED)
        env.seed(_params_rl.SEED)
        np.random.seed(_params_rl.SEED)

    # initialize logging variables
    returns = []
    average_returns = []
    policy_iterations = 0
    n_interactions = 0

    # training loop
    for n_epi in range(int(_params_rl.N_EPISODES + 1)):
        env.reset()
        lstm_in = ppo.model.reset_lstm()
        done = False
        ret = 0
        obs = env.get_observation()

        while not done:
            with torch.no_grad():

                # run policy
                action, action_logprob, state_value, lstm_out = ppo.select_action(torch.FloatTensor(obs), lstm_in)
                next_obs, reward, done, _ = env.step(np.array([action.item(), 0.9]))

                # store data in ppo.buffer
                ppo.buffer.store(obs=torch.FloatTensor(obs).to(device), act=action.flatten(),
                                 rew=torch.FloatTensor([reward/100]), val=torch.FloatTensor([state_value]),
                                 logp=action_logprob, lstm_h_in=lstm_in[0].flatten(),
                                 lstm_c_in=lstm_in[1].flatten(), done=torch.FloatTensor([done]))

                # update variables each interaction
                obs = next_obs
                lstm_in = lstm_out
                ret += reward
                n_interactions += 1

            # update policy every BATCHSIZE tuples stored in buffer
            if n_interactions != 0 and n_interactions % _params_rl.BATCHSIZE == 0:
                next_value = ppo.model.critic(torch.FloatTensor(obs)).detach()
                ppo.buffer.finish_path(next_value.flatten())
                ppo.model.train()
                ppo.update()
                ppo.model.eval()
                n_interactions = 0
                policy_iterations += 1

            # stop rollout if episode is completed
            if done:
                ppo.buffer.finish_path(torch.FloatTensor([0]).to(device))
                returns.append(ret)
                break

        n_mean = _params_logging.PRINT_INTERVAL if len(returns) >= _params_logging.PRINT_INTERVAL else len(returns)
        average_returns.append(np.convolve(returns[-n_mean:], np.ones((n_mean,)) / n_mean, mode='valid')[0])

        if n_epi % _params_logging.PRINT_INTERVAL == 0:
            print("# episode: {}/{} \t\t avg. ret. (last {} eps): {:06.1f}"
                  .format(n_epi, int(_params_rl.N_EPISODES), _params_logging.PRINT_INTERVAL, average_returns[-1]))
            with open("returnFile_running.dat", "a+") as returnFile:
                returnFile.write(format(n_epi) + "," + format(policy_iterations) + "," +
                                 '{:.1f}'.format(average_returns[-1]) + "\n")

        if n_epi % _params_logging.SAVE_INTERVAL == 0:
            torch.save(ppo.model.actor.state_dict(), "updraft_exploiter_actor_episode_{}".format(n_epi) + ".pt")
            torch.save(ppo.model.critic.state_dict(), "updraft_exploiter_critic_episode_{}".format(n_epi) + ".pt")
            evaluate_updraft_exploiter.main(env, ppo, n_epi, _params_agent, validation_mask=True)

    # display results
    now = datetime.datetime.now()
    plt.figure("average returns")
    plt.plot(average_returns)
    plt.ylabel("average returns (-)")
    plt.xlabel("episodes (-)")
    plt.grid(True)
    plt.savefig("average_returns_" + now.strftime("%d-%B-%Y_%H-%M") + ".png")
    plt.show()

    # save actor-critic
    torch.save(ppo.model.actor.state_dict(), "updraft_exploiter_actor_final_" + now.strftime("%d-%B-%Y_%H-%M") + ".pt")
    torch.save(ppo.model.critic.state_dict(), "updraft_exploiter_critic_final_" + now.strftime("%d-%B-%Y_%H-%M") + ".pt")

    env.close()


if __name__ == '__main__':
    main()
