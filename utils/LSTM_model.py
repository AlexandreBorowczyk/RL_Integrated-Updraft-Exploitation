import torch
import torch.nn as nn
from torch.distributions import Normal


class ActorCritic(nn.Module):
    def __init__(self, params_model, params_rl, device=torch.device("cpu")):
        super().__init__()

        # instantiate parameters
        self._params_model = params_model
        self._params_rl = params_rl
        self._device = device

        # setup ANN
        self.actor = LSTMActor(obs_dim=self._params_model.DIM_IN, act_dim=self._params_model.DIM_OUT,
                               hidden_size=self._params_model.DIM_HIDDEN, lstm_size=self._params_model.DIM_LSTM,
                               device=self._device)
        self.critic = Critic(obs_dim=self._params_model.DIM_IN, hidden_size=self._params_model.DIM_HIDDEN,
                             device=self._device)

    def act(self, state, lstm_hidden, validation_mask=False):
        # evaluate current actor to sample action for rollout
        action_mean, lstm_hidden = self.actor.forward(state, lstm_hidden)
        dist = Normal(action_mean, self._params_rl.SIGMA * (not validation_mask))
        action = dist.sample()
        action_logprob = dist.log_prob(action).mean()

        return action, action_logprob, lstm_hidden

    def evaluate_actor(self, sampled_state, sampled_action, sampled_lstm_hidden):
        # evaluate actor for sampled states
        action_mean, _ = self.actor.forward(sampled_state, sampled_lstm_hidden)
        dist = Normal(torch.flatten(action_mean, 1), self._params_rl.SIGMA)

        # get logprobs for distribution subject to current actor, evaluated for sampled actions
        action_logprobs = dist.log_prob(sampled_action)

        return torch.mean(action_logprobs, 1)

    def reset_lstm(self):
        return torch.zeros(1, 1, self._params_model.DIM_LSTM, device=self._device), \
               torch.zeros(1, 1, self._params_model.DIM_LSTM, device=self._device)


class LSTMActor(nn.Module):
    def __init__(self, obs_dim, act_dim, hidden_size, lstm_size, device):
        super().__init__()

        self.input_layer = nn.Linear(obs_dim, hidden_size)
        self.lstm = nn.LSTM(hidden_size, lstm_size)
        self.output_layer = nn.Linear(lstm_size, act_dim)

        self._obs_dim = obs_dim
        self._act_dim = act_dim
        self._hidden_size = hidden_size
        self._lstm_size = lstm_size

        self._device = device

    def forward(self, observation, lstm_hidden):
        # evaluate input
        x = observation.reshape(-1, self._obs_dim).to(self._device)  # seq_len x  input_size
        x = torch.tanh(self.input_layer(x))

        # evaluate lstm
        x = x.reshape(-1, 1, self._hidden_size)  # seq_len x batch_size x  lstm_in_size
        x, lstm_hidden = self.lstm(x, (lstm_hidden[0].reshape(1, 1, self._hidden_size),
                                       lstm_hidden[1].reshape(1, 1, self._hidden_size)))

        # evaluate actor output layer
        x = x.reshape(-1, self._lstm_size)  # seq_len x lstm_out_size
        action = self.output_layer(x)

        return action, lstm_hidden


class Critic(nn.Module):
    def __init__(self, obs_dim, hidden_size, device):
        super().__init__()

        self.input_layer = nn.Linear(obs_dim, hidden_size)
        self.hidden_layer = nn.Linear(hidden_size, hidden_size)
        self.output_layer = nn.Linear(hidden_size, 1)

        self._obs_dim = obs_dim

        self._device = device

    def forward(self, observation):
        # evaluate input
        x = observation.reshape(-1, self._obs_dim).to(self._device)  # batch_size x  input_size
        x = torch.tanh(self.input_layer(x))

        # evaluate hidden layer
        x = torch.tanh(self.hidden_layer(x))

        # evaluate critic output layer
        value = self.output_layer(x)

        return value
