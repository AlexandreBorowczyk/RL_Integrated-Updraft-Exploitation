import torch
from torch.utils.data import Dataset
#import utils.Spinup_core as core
import scipy.signal

class PPOBuffer:
    """ Partly adopted from 'Spinning Up in Deep Reinforcement Learning' (2018) by Joshua Achiam, OpenAI
        https://github.com/openai/spinningup/blob/master/spinup/algos/pytorch/ppo/ppo.py
        A buffer for storing trajectories experienced by a PPO agent interacting
        with the environment, and using Generalized Advantage Estimation (GAE-Lambda)
        for calculating the advantages of state-action pairs.

        Attributes
        ----------
        obs_buf : Tensor
            Observation buffer

        act_buf : Tensor
            Action buffer

        adv_buf : Tensor
            Advantage estimation buffer

        rew_buf : Tensor
            Reward buffer

        ret_buf : Tensor
            Return buffer

        val_buf : Tensor
            State value buffer

        logp_buf : Tensor
            Log probability buffer

        lstm_h_in_buf : Tensor
            LSTM hidden state buffer

        lstm_c_in_buf : Tensor
            LSTM cell state buffer

        done_buf : Tensor
            Done flag buffer

        gamma : float
            Discount factor

        lam : float
            Lambda for Generalized Advantage Estimation (GAE)

        ptr: int
            Pointer to indicate storage position in buffer

        path_start_idx : int
            Start index of current trajectory

        max_size : int
            Maximum buffe size

        _device : torch.device
            Sets training device (CPU or GPU)
    """

    def __init__(self, obs_dim, act_dim, batch_size, lstm_hidden_size, gamma=0.99, lam=0.95,
                 device=torch.device("cpu")):
        self.obs_buf = torch.zeros(batch_size, obs_dim, dtype=torch.float32, device=device)
        self.act_buf = torch.zeros(batch_size, act_dim, dtype=torch.float32, device=device)
        self.adv_buf = torch.zeros(batch_size, dtype=torch.float32, device=device)
        self.rew_buf = torch.zeros(batch_size, dtype=torch.float32, device=device)
        self.ret_buf = torch.zeros(batch_size, dtype=torch.float32, device=device)
        self.val_buf = torch.zeros(batch_size, dtype=torch.float32, device=device)
        self.logp_buf = torch.zeros(batch_size, dtype=torch.float32, device=device)
        self.lstm_h_in_buf = torch.zeros(batch_size, lstm_hidden_size, dtype=torch.float32, device=device)
        self.lstm_c_in_buf = torch.zeros(batch_size, lstm_hidden_size, dtype=torch.float32, device=device)
        self.done_buf = torch.zeros(batch_size, dtype=torch.bool, device=device)
        self.gamma, self.lam = gamma, lam
        self.ptr, self.path_start_idx, self.max_size = 0, 0, batch_size
        self._device = device

    def store(self, obs, act, rew, val, logp, lstm_h_in, lstm_c_in, done):
        """ Append one timestep of agent-environment interaction to the buffer at pointer position

            Parameters
            ----------
            obs : Tensor
                Observation at timestep

            act : Tensor
                Action at timestep

            rew : float
                Reward at timestep

            val : float
                State value at timestep

            logp : Tensor
                Log probability at timestep

            lstm_h_in :
                LSTM hidden state at timestep

            lstm_c_in :
                LSTM cell state at timestep

            done : bool
                Done flag at timestep
        """

        assert self.ptr < self.max_size  # buffer has to have room so you can store
        self.obs_buf[self.ptr] = obs
        self.act_buf[self.ptr] = act
        self.rew_buf[self.ptr] = rew
        self.val_buf[self.ptr] = val
        self.logp_buf[self.ptr] = logp
        self.lstm_h_in_buf[self.ptr] = lstm_h_in
        self.lstm_c_in_buf[self.ptr] = lstm_c_in
        self.done_buf[self.ptr] = done
        self.ptr += 1

    def finish_path(self, last_val=0):
        """ Call this at the end of a trajectory, or when one gets cut off by an epoch ending. This looks back in the
            buffer to where the trajectory started, and uses rewards and value estimates from the whole trajectory to
            compute advantage estimates with GAE-Lambda, as well as compute the rewards-to-go for each state, to use as
            the targets for the value function.
            The "last_val" argument should be 0 if the trajectory ended because the agent reached a terminal
            state (died), and otherwise should be V(s_T), the value function estimated for the last state.
            This allows us to bootstrap the reward-to-go calculation to account for timesteps beyond the arbitrary
            episode horizon (or epoch cutoff).

            Parameters
            ----------
            last_val : float
                Reward and state value for terminal state
        """

        path_slice = slice(self.path_start_idx, self.ptr)
        rews = torch.cat((self.rew_buf[path_slice], last_val))
        vals = torch.cat((self.val_buf[path_slice], last_val))

        # the next two lines implement GAE-Lambda advantage calculation
        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]
        self.adv_buf[path_slice] = torch.FloatTensor(
            (self.discount_cumsum(deltas.cpu().numpy(), self.gamma * self.lam)).copy()).to(self._device)

        # the next line computes rewards-to-go, to be targets for the value function
        self.ret_buf[path_slice] = torch.FloatTensor(
            (self.discount_cumsum(rews.cpu().numpy(), self.gamma)[:-1]).copy()).to(self._device)

        self.path_start_idx = self.ptr

    def get(self, ):
        """ Gets all of the data from the buffer and resets pointer.

            Returns
            -------
            return_data : dictionary
                Dictionary with content of PPO buffer. Keys are: obs, act, ret, adv, logp, lstm_h_in, lstm_c_in, done with
                the corresponding values from buffer
        """
        assert self.ptr == self.max_size  # buffer has to be full before you can get
        self.ptr, self.path_start_idx = 0, 0

        # the next line implements the advantage normalization trick
        self.adv_buf = (self.adv_buf - self.adv_buf.mean()) / (self.adv_buf.std() + 1e-5)

        data = dict(obs=self.obs_buf, act=self.act_buf, ret=self.ret_buf, adv=self.adv_buf, logp=self.logp_buf,
                    lstm_h_in=self.lstm_h_in_buf, lstm_c_in=self.lstm_c_in_buf, done=self.done_buf)
        return {k: torch.as_tensor(v, dtype=torch.float32, device=self._device) for k, v in data.items()}

    @staticmethod
    def discount_cumsum(x, discount):
        """ Function taken from https://github.com/openai/spinningup/blob/master/spinup/algos/pytorch/ppo/core.py

            input:
                vector x,
                [x0,
                 x1,
                 x2]

            output:
                [x0 + discount * x1 + discount^2 * x2,
                 x1 + discount * x2,
                 x2]
        """
        return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]


class SlidingWindowDataset(Dataset):
    """ Extends pytorch's Dataset class to create a sliding window
        dataset from the PPO buffer data

        Attributes
        ----------
        data: TensorDataset
            Data from PPO buffer
        window: int
            Length of sequence, which is used for stochastic gradient descent (SGD)
    """
    def __init__(self, data, window):
        self.data = data
        self._window = window

    def __getitem__(self, index):
        x = self.data[index:index + self._window]
        return x

    def __len__(self):
        return len(self.data) - self._window
