import datetime
import os
import shutil
import gym
import torch
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import collections
import matplotlib.gridspec as gridspec
from matplotlib import cm
import warnings

from parameters import params_environment, params_triangle_soaring, params_updraft_finder
from policy_training.PPO_updraft_finder import PPO

# import glider package to correctly register environment
import glider

# set training device
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


def plot_policy_sample(env, controller, n_iter, params_agent, validation_mask=False):
    """ Evaluates updraft finder in given environment and plots result

        Parameters
        ----------
        env : GliderEnv3D
            Custom gym environment for glider simulation

        controller : PPO
            Updraft finder policy

        n_iter : int
            Number of iterations

        params_agent : AgentParameters
            Parameters for updraft finder

        validation_mask : boolean
            Flag to enable sampling of action with std-deviation for exploration
        """

    # initialize environment
    state = env.reset()
    obs = env.get_observation()
    lstm_hidden_in = controller.model.reset_lstm()
    done = False
    ret = 0

    # initialize logger for position and control
    pos_list = [[state[0], state[1], state[2]]]
    ctrl_list = []
    updraft_position_init = np.copy(env._wind_fun._wind_data['updraft_position'])

    # set params
    _params_task = params_triangle_soaring.TaskParameters()
    _params_sim = params_environment.SimulationParameters()
    _params_wind = params_environment.WindParameters()

    # run one episode in environment
    while not done:
        # evaluate and apply policy
        action, _, _, lstm_hidden_out = controller.select_action(torch.FloatTensor(obs), lstm_hidden_in,
                                                                 validation_mask=validation_mask)
        obs, r, done, info = env.step(np.array([[action.item(), 1]]))
        ret += r
        lstm_hidden_in = lstm_hidden_out

        # write to lists for Logging
        pos_list.append([env.state[0], env.state[1], env.state[2]])
        control = env.action2control(np.array([action.detach().cpu().flatten(), 1]))
        ctrl_list.append([control[0], control[1]])

    time = env.time

    # plot position and control trajectory
    fig = plt.figure()
    fig.set_size_inches(11.69, 8.27)  # DinA4
    fig.suptitle("Sample after {} iterations of training:\nReturn: {:.1f}, Wind: {:.1f} m/s from {:.1f}Â°"
                 .format(n_iter, ret, env._wind_fun._wind_data['headwind_velocity'],
                         np.rad2deg(env._wind_fun._wind_data['headwind_direction'])))

    """ The next section generates 4 subplots, which show the North-East trajectory, mu_cmd, alpha_cmd and
            height over time.

            ax1 : NE-trajectory
            ax2 : height over time
            ax3 : mu_cmd over time
            ax4 : alpha_cmd over time
        """

    grid = gridspec.GridSpec(ncols=2, nrows=3, figure=fig)
    ax1 = fig.add_subplot(grid[0:2, 0])
    ax2 = fig.add_subplot(grid[-1, :])
    ax3 = fig.add_subplot(grid[0, 1])
    ax4 = fig.add_subplot(grid[1, 1])

    # plot updraft position trajectory
    updraft_position_final = env._wind_fun._wind_data['updraft_position']
    if not (np.isnan(updraft_position_init).any() or np.isnan(updraft_position_final).any()):
        for k in range(len(updraft_position_init[0])):
            updraft_init_outline = plt.Circle((updraft_position_init[1, k], updraft_position_init[0, k]),
                                              _params_wind.DELTA,
                                              color='b', fill=False)
            updraft_final_outline = plt.Circle((updraft_position_final[1, k], updraft_position_final[0, k]),
                                               _params_wind.DELTA,
                                               color='r', fill=False)
            ax1.add_artist(updraft_init_outline)
            ax1.add_artist(updraft_final_outline)
            ax1.arrow(updraft_position_init[1, k], updraft_position_init[0, k],
                      (updraft_position_final[1, k] - updraft_position_init[1, k]),
                      (updraft_position_final[0, k] - updraft_position_init[0, k]), head_width=20)

    # # scale colormap for time axis
    timeVec = np.linspace(params_agent.TIMESTEP_CTRL, time, len(pos_list))
    colormap = cm.spring(timeVec / timeVec.max())

    # plot solid north-east trajectory with color gradient
    for k in range(len(colormap)):
        x_segm = np.array(pos_list)[k:(k + 2), 1]
        y_segm = np.array(pos_list)[k:(k + 2), 0]
        c_segm = colormap[k]
        ax1.plot(x_segm, y_segm, c=c_segm)

    ax1.set_xlim(-params_agent.DISTANCE_MAX, params_agent.DISTANCE_MAX)
    ax1.set_ylim(-params_agent.DISTANCE_MAX, params_agent.DISTANCE_MAX)
    ax1.set_xlabel("east (m)")
    ax1.set_ylabel("north (m)")
    ax1.grid(True)

    # plot solid height trajectory with color gradient
    for k in range(len(colormap)):
        x_segm = timeVec[k:(k + 2)]
        y_segm = -np.array(pos_list)[k:(k + 2), 2]
        c_segm = colormap[k]
        ax2.plot(x_segm, y_segm, c=c_segm)

    ax2.set_xlim(0, params_agent.DURATION_MAX)
    ax2.set_ylim(0, params_agent.HEIGHT_MAX)
    ax2.set_xlabel("time (s)")
    ax2.set_ylabel("height (m)")
    ax2.grid(True)

    ax3.scatter(timeVec[1:], (180 / np.pi) * np.array(ctrl_list)[:, 0], s=2, c=colormap[1:], edgecolor='none')
    ax3.set_xlim(0, params_agent.DURATION_MAX)
    ax3.set_ylim(-45, 45)
    ax3.set_xticklabels([])
    ax3.set_ylabel("mu (deg)")
    ax3.grid(True)

    ax4.scatter(timeVec[1:], (180 / np.pi) * np.array(ctrl_list)[:, 1], s=2, c=colormap[1:], edgecolor='none')
    ax4.set_xlim(0, params_agent.DURATION_MAX)
    ax4.set_ylim(0, 12)
    ax4.set_xlabel("time (s)")
    ax4.set_ylabel("alpha (deg)")
    ax4.grid(True)
    ax4.get_shared_x_axes().join(ax4, ax3)

    warnings.filterwarnings("ignore", category=UserWarning, module="backend_interagg")
    grid.tight_layout(fig, rect=[0, 0.03, 1, 0.95])
    plt.savefig("resultant_trajectory_episode_{}".format(n_iter) + ".png", dpi=400)
    plt.show()

    plt.close(fig)
    env.close()


def create_experiment_folder(_params_rl, _params_agent, _params_logging):
    """ Creates new folder to store training results

    Parameters
    ----------
    _params_rl : LearningParameters
        Hyperparameters for training actor-critic model

    _params_agent : AgentParameters
        Parameters for initializing and simulation agent (glider)

    _params_logging : LoggingParameters
        Intervals for saving and logging

    Returns
    -------
    parameterFile : io.TextIOWrapper
        File descriptor to parameter file

    returnFile : io.TextIOWrapper
        File descriptor to return file
    """

    # create folder to store data for the experiment running
    experimentID = 1
    dirName = "{}_updraft_finder_experiment".format(experimentID)
    while os.path.exists(dirName):
        experimentID += 1
        dirName = "{}_".format(experimentID) + dirName.split('_', 1)[1]
    os.mkdir(dirName)
    shutil.copytree(os.getcwd(), os.path.join(dirName, "Sources_unzipped"),
                    ignore=shutil.ignore_patterns('*experiment*', 'archive', 'tests', '.git*', 'rl_ccs_experiments',
                                                  '../.idea', '__pycache__', 'README*'))

    os.chdir(dirName)
    shutil.make_archive("Sources", 'zip', "Sources_unzipped")
    shutil.rmtree("Sources_unzipped")
    print("Directory for running experiment no. {} created".format(experimentID))

    # save parameters to file
    parameterFile = open("parameterFile.txt", "w")
    parameterFile.write(
        format(vars(_params_rl)) + "\n" +
        format(vars(params_updraft_finder.ModelParameters())) + "\n" +
        format(vars(_params_agent)) + "\n" +
        format(vars(_params_logging)) + "\n\n" +
        format(vars(params_triangle_soaring.TaskParameters())) + "\n\n" +
        format(vars(params_environment.SimulationParameters())) + "\n" +
        format(vars(params_environment.GliderParameters())) + "\n" +
        format(vars(params_environment.PhysicsParameters())) + "\n" +
        format(vars(params_environment.WindParameters())))
    parameterFile.close()

    # set up file to save average returns and scores
    returnFile = open("returnFile_running.dat", "w")
    returnFile.write("iterations,episodes,avg_returns\n")
    returnFile.close()

    return parameterFile, returnFile


def run_updraft_finder_training():
    # set up training
    env = gym.make('glider3D-v0', agent='updraft_finder')

    # instantiate agent
    ppo = PPO()

    # load parameters
    _params_rl = params_updraft_finder.LearningParameters()
    _params_agent = params_updraft_finder.AgentParameters()
    _params_logging = params_updraft_finder.LoggingParameters()

    parameterFile, returnFile = create_experiment_folder(_params_rl, _params_agent, _params_logging)

    # set random seed
    if _params_rl.SEED:
        print("Random Seed: {}".format(_params_rl.SEED))
        torch.manual_seed(_params_rl.SEED)
        env.seed(_params_rl.SEED)
        np.random.seed(_params_rl.SEED)

    # initialize logging variables
    returns = collections.deque(maxlen=10)
    average_returns = []
    policy_iterations = 0
    episodes = 0
    interactions = 0
    ret = 0

    # create sample of untrained system behavior
    plot_policy_sample(env, ppo, policy_iterations, _params_agent, validation_mask=True)

    # (re-)set env and model
    env.reset()
    obs = env.get_observation()
    lstm_in = ppo.model.reset_lstm()

    # training loop
    while policy_iterations < (int(_params_rl.N_ITERATIONS)):
        # rollout s.t. current policy
        with torch.no_grad():
            action, action_logprob, state_value, lstm_out = \
                ppo.select_action(torch.FloatTensor(obs), lstm_in)
            next_obs, reward, done, _ = env.step(np.array([[action.item(), 1]]))

        # store data in ppo.buffer
        ppo.buffer.store(obs=torch.FloatTensor(obs).to(device), act=action.flatten(),
                         rew=torch.FloatTensor([reward / 100]), val=torch.FloatTensor([state_value]),
                         logp=action_logprob.flatten(), lstm_h_in=lstm_in[0].flatten(),
                         lstm_c_in=lstm_in[1].flatten(), done=torch.FloatTensor([done]))

        # update variables each interaction
        obs = next_obs
        lstm_in = lstm_out
        ret += reward
        interactions += 1

        # store results and reset experiment if episode is completed
        if done:
            ppo.buffer.finish_path(torch.FloatTensor([0]).to(device))
            returns.append(ret)
            episodes += 1
            ret = 0
            env.reset()
            lstm_in = ppo.model.reset_lstm()
            obs = env.get_observation()

            n_mean = _params_logging.PRINT_INTERVAL if len(returns) >= _params_logging.PRINT_INTERVAL else len(returns)
            average_returns.append(np.convolve(list(returns)[-n_mean:], np.ones((n_mean,)) / n_mean, mode='valid')[0])

        # update policy every BATCHSIZE interactions
        if interactions != 0 and interactions % _params_rl.BATCHSIZE == 0:
            next_value = ppo.model.critic(torch.FloatTensor(obs)).detach()
            ppo.buffer.finish_path(next_value.flatten())
            ppo.model.train()
            ppo.update()
            ppo.model.eval()
            interactions = 0
            policy_iterations += 1

            # print results to display and log them to file
            if len(average_returns) and policy_iterations % _params_logging.PRINT_INTERVAL == 0:
                print("# policy iteration: {}/{} \t\t"
                      "avg. return over last 10 episodes: {:06.1f}"
                      .format(policy_iterations, int(_params_rl.N_ITERATIONS), average_returns[-1]))

                with open("returnFile_running.dat", "a+") as return_file:
                    return_file.write(format(policy_iterations) + "," + format(episodes) + "," +
                                      '{:.1f}'.format(average_returns[-1]) + "\n")

            # save model and create sample of system behavior
            if policy_iterations % _params_logging.SAVE_INTERVAL == 0:
                torch.save(ppo.model.actor.state_dict(),
                           "updraft_finder_actor_iter_{}".format(policy_iterations) + ".pt")
                torch.save(ppo.model.critic.state_dict(),
                           "updraft_finder_critic_iter_{}".format(policy_iterations) + ".pt")
                plot_policy_sample(env, ppo, policy_iterations, _params_agent, validation_mask=True)

                fig, ax = plt.subplots()
                returns_to_plot = pd.read_csv('returnFile_running.dat')
                returns_to_plot.plot(x='iterations', y='avg_returns', ax=ax)
                plt.title("learning success")
                plt.xlabel("policy iterations (-)")
                plt.ylabel("average return")
                plt.grid(True)
                plt.savefig("average_returns_{}".format(policy_iterations) + ".png")
                plt.show()

    # save final model
    now = datetime.datetime.now()
    torch.save(ppo.model.actor.state_dict(), "updraft_finder_actor_final_" + now.strftime("%d-%B-%Y_%H-%M") + ".pt")
    torch.save(ppo.model.critic.state_dict(), "updraft_finder_critic_final_" + now.strftime("%d-%B-%Y_%H-%M") + ".pt")

    # rename parameter file consistently
    os.rename(parameterFile.name, "parameters_" + now.strftime("%d-%B-%Y_%H-%M") + ".txt")

    # rename return file consistently
    return_file.close()
    os.rename(return_file.name, "average_returns_" + now.strftime("%d-%B-%Y_%H-%M") + ".dat")

    env.close()


if __name__ == '__main__':
    run_updraft_finder_training()
