import numpy as np
from parameters.params_environment import GliderParameters, PhysicsParameters


class LearningParameters:
    """ Hyperparameters for training actor-critic model

    Attributes
    ----------

    N_ITERATIONS: float
        total number of policy iterations during training

    BATCHSIZE: int
        size of bach (i.e, length of rollout) before policy update

    SEQ_LEN: int
        size of mini-batch for SGD (evaluated as a sequence, due to LSTM)

    OVERLAP: int
        shift tuples for overlapping sequences (shift = seq_len - overlap)

    SEQ_LEN_MIN: int
        use first steps of each sequence to "burn in" hidden state

    N_BURN_IN: int
        use first steps of each sequence to "burn in" hidden state

    K_EPOCH: int
        number of policy updates on single batch

    LEARNING_RATE_PI: float
        learning rate for actor optimizer

    LEARNING_RATE_VF: float
        learning rate for critic optimizer

    GAMMA: float
        discount factor for advantage estimation

    LAMBDA: float
        only relevant if GAE is implemented

    EPS_CLIP: float
        PPO clipping value

    SIGMA: float
        std-deviation for exploration (0.1 -> 0.6 deg after scaling)

    AUTO_EXPLORATION: bool
        exploration driven by NN output (self.SIGMA obsolete if true)

    SEED: None
        manual specification of random seed (Fabian: 42)
"""

    def __init__(self):
        self.N_ITERATIONS = 4e3  # total number of policy iterations during training
        self.BATCHSIZE = 4096  # size of bach (i.e, length of rollout) before policy update
        self.SEQ_LEN = 256  # size of mini-batch for SGD (evaluated as a sequence, due to LSTM)
        self.OVERLAP = 128  # shift tuples for overlapping sequences (shift = seq_len - overlap)
        self.SEQ_LEN_MIN = 64  # minimum sequence length to be evaluated (if sliced due to episode end)
        self.N_BURNIN = 16  # use first steps of each sequence to "burn in" hidden state
        self.K_EPOCH = 10  # number of policy updates on single batch
        self.LEARNING_RATE_PI = 1e-5  # learning rate for actor optimizer
        self.LEARNING_RATE_VF = 1e-4  # learning rate for critic optimizer
        self.GAMMA = 0.99  # discount factor for advantage estimation
        self.LAMBDA = 0.96  # only relevant if GAE is implemented
        self.EPS_CLIP = 0.2  # PPO clipping value
        self.SIGMA = .2  # std-deviation for exploration (0.1 -> 0.6 deg after scaling)
        self.AUTO_EXPLORATION = False  # exploration driven by NN output (self.SIGMA obsolete if true)
        self.SEED = None  # manual specification of random seed


class ModelParameters:
    """ Parameters which describe ANN architecture

           Attributes
           ----------

           DIM_IN: int
               Dimension of input layer (observation-space)

           DIM_OUT: int
               Dimension of output layer (action-space). For decision maker: prob. for subtask

           DIM_HIDDEN: int
               Dimension of hidden layer

           DIM_LSTM: int
               Dimension of LSTM layer
    """

    def __init__(self):
        self.DIM_IN = 4  # dimension of observation-space
        self.DIM_OUT = 1  # dimension of action-space - for decision maker: prob. for subtask
        self.DIM_HIDDEN = 32
        self.DIM_LSTM = 32


class AgentParameters:
    """ Parameters for initializing and simulation agent (glider)

            Attributes
            ----------

            TIMESTEP_CTRL: float
                Control update time-step [s]

            INITIAL_SPACE: ndarray
                initial state space [MIN MAX]

            ACTION_SPACE: ndarray
                bank angle and AoA constraint to [MIN MAX]  (deg)

            DISTANCE_MAX: float
                maximum horizontal distance allowed from origin (ensures that a safety pilot could interact)

            HEIGHT_MAX: int
                maximum height allowed
    """

    def __init__(self):
        # instantiate params_glider, params_physics object
        self._params_glider = GliderParameters()
        self._params_physics = PhysicsParameters()

        # control update time-step (s)
        self.TIMESTEP_CTRL = 1.0

        # initial state space [MIN MAX]
        self.INITIAL_SPACE = np.array([[-500, 500],
                                       [-500, 500],
                                       [-405, -395],
                                       [-10, 10],
                                       [-10, 10],
                                       [0.5, 1.5]])

        # bank angle and AoA constraint to [MIN MAX]   (deg)
        self.ACTION_SPACE = np.array([[-45, 45],
                                      [0, 12]])

        # mean values for observation standardization
        self.OBS_MEAN = np.array([0, 0, -250,  # position: NED
                                  0])  # energy equivalent climb rate (TEK vario)

        # spread parameters for observation standardization
        self.OBS_STD = np.array([500, 500, 250, 4])

        # maximum horizontal distance allowed from origin (at the moment: used for plotting, only)
        self.DISTANCE_MAX = 2e3

        # maximum height allowed (at the moment: used for plotting, only)
        self.HEIGHT_MAX = 1e3

        # maximum episode duration
        self.DURATION_MAX = 1800


class LoggingParameters:
    """ Intervals for saving and logging

        Attributes
        ----------

        PRINT_INTERVAL: int
            Interval wrt # epi to print score and save avg. return to file
        SAVE_INTERVAL: int
            Interval wrt # epi to save actor/critic net and make a plot
        """

    def __init__(self):
        self.PRINT_INTERVAL = 10  # interval wrt # epi to print score and save avg. return to file
        self.SAVE_INTERVAL = 500  # interval wrt # epi to save actor/critic net and make a plot
