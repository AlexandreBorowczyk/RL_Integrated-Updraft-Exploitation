import gym
import glider
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import normal, MultivariateNormal
import numpy as np
import datetime
import matplotlib.pyplot as plt

from params import *
import evaluateGlider

# device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
device = torch.device("cpu")

class Memory:
    def __init__(self):
        self.actions = []
        self.states = []
        self.logprobs = []
        self.rewards = []

    def clear_memory(self):
        del self.actions[:]
        del self.states[:]
        del self.logprobs[:]
        del self.rewards[:]


class ActorCritic(nn.Module):
    def __init__(self):
        super(ActorCritic, self).__init__()

        self._params_model = params_model()
        self._params_rl = params_rl()

        self.hiddenLayer_in = nn.Linear(self._params_model.DIM_IN, self._params_model.DIM_HIDDEN)
        self.hiddenLayer_internal = nn.Linear(self._params_model.DIM_HIDDEN, self._params_model.DIM_HIDDEN)
        self.outputLayer_actor = nn.Linear(self._params_model.DIM_HIDDEN,
                                           self._params_model.DIM_OUT * (self._params_rl.AUTO_EXPLORATION + 1))
        self.outputLayer_critic = nn.Linear(self._params_model.DIM_HIDDEN, 1)

        self.action_var = torch.full((self._params_model.DIM_OUT,),
                                     self._params_rl.SIGMA * self._params_rl.SIGMA).to(device)

    def actor(self, x):
        x = torch.tanh(self.hiddenLayer_in(x))
        for ii in range(self._params_model.NUM_HIDDEN - 1):
            x = torch.tanh(self.hiddenLayer_internal(x))
        x = self.outputLayer_actor(x)

        pi = torch.Tensor(self._params_model.DIM_OUT * (self._params_rl.AUTO_EXPLORATION + 1))
        pi[0:self._params_model.DIM_OUT] = torch.tanh(x[0:self._params_model.DIM_OUT])
        if self._params_rl.AUTO_EXPLORATION:
            pi[self._params_model.DIM_OUT] = x[self._params_model.DIM_OUT]
        return pi

    def critic(self, x):
        x = torch.tanh(self.hiddenLayer_in(x))
        for ii in range(self._params_model.NUM_HIDDEN - 1):
            x = torch.tanh(self.hiddenLayer_internal(x))
        v = self.outputLayer_critic(x)
        return v

    def act(self, state, memory, validation=False):
        pi = self.actor(state)
        action_mean = pi[0:self._params_model.DIM_OUT]

        if self._params_rl.AUTO_EXPLORATION and (not validation):
            action_logStd = pi[-self._params_model.DIM_OUT:]
            action_var = torch.exp(action_logStd)**2
            action_var = action_var.reshape(-1) #TODO assure that this works for DIM_OUT > 1
        elif (not self._params_rl.AUTO_EXPLORATION) and (not validation):
            action_var = self.action_var
        else:
            action_var = torch.full((self._params_model.DIM_OUT,), 1e-6)

        dist = MultivariateNormal(action_mean, torch.diag(action_var).to(device))
        action = dist.sample()
        action_logprob = dist.log_prob(action)

        memory.states.append(state)
        memory.actions.append(action)
        memory.logprobs.append(action_logprob)

        return action.detach()

    def evaluate(self, states, actions):
        action_mean_list = []
        state_value_list = []
        for idx in range(len(states)):
            pi = self.actor(states[idx])
            action_mean = pi[0:self._params_model.DIM_OUT]
            action_mean_list.append(action_mean)
            state_value_list.append(self.critic(states[idx]))
        action_mean_list = torch.stack(action_mean_list).view(-1, 1, self._params_model.DIM_OUT)
        state_value_list = torch.stack(state_value_list).view(-1, 1, 1)

        if self._params_rl.AUTO_EXPLORATION:
            action_logStd = pi[self._params_model.DIM_OUT]
            action_var = torch.exp(action_logStd)**2
            action_var = action_var.reshape(-1) #TODO assure that this works for DIM_OUT > 1
        else:
            action_var = self.action_var

        dist = MultivariateNormal(torch.squeeze(action_mean_list, 1), torch.diag(action_var))
        action_logprobs = dist.log_prob(actions)
        dist_entropy = dist.entropy()

        return action_logprobs, torch.squeeze(state_value_list), dist_entropy

class PPO:
    def __init__(self):
        self._params_model = params_model()
        self._params_rl = params_rl()

        self.policy = ActorCritic().to(device)
        self.policy_old = ActorCritic().to(device)
        # self.policy.load_state_dict(torch.load("actor_critic_network_final_06-November-2019_14-58.pt"))
        # self.policy_old.load_state_dict(torch.load("actor_critic_network_final_06-November-2019_14-58.pt"))

        self.optimizer = torch.optim.Adam(self.policy.parameters(),
                                          lr=self._params_rl.LEARNING_RATE, betas=(0.9, 0.999))
        self.MseLoss = nn.MSELoss()

    def update(self, memory):

        # set training mode for ANN
        self.policy.train()
        self.policy_old.train()

        # discount sampled reward
        discounted_returns_sampled = []
        discounted_return = 0
        for reward in reversed(memory.rewards):
            discounted_return = reward + (self._params_rl.GAMMA * discounted_return)
            discounted_returns_sampled.insert(0, discounted_return)

        # normalize
        discounted_returns_sampled = torch.tensor(discounted_returns_sampled).to(device)
        discounted_returns_sampled = (discounted_returns_sampled - discounted_returns_sampled.mean()) \
                                     / (discounted_returns_sampled.std() + 1e-5)

        # convert list to tensor
        old_states = memory.states
        old_actions = torch.stack(memory.actions).to(device).detach()
        old_logprobs = torch.squeeze(torch.stack(memory.logprobs)).to(device).detach()

        ##### K_EPOCH times policy optimization ######
        for _ in range(self._params_rl.K_EPOCH):
            # evaluation of old states and actions
            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)

            # ppo ratio
            ratios = torch.exp(logprobs - old_logprobs.detach())

            # surrogate loss
            advantages = discounted_returns_sampled - state_values.detach()
            surr1 = ratios * advantages
            surr2 = torch.clamp(ratios, 1 - self._params_rl.EPS_CLIP, 1 + self._params_rl.EPS_CLIP) * advantages
            loss = -torch.min(surr1, surr2) \
                   + 0.5 * self.MseLoss(state_values, discounted_returns_sampled) \
                   - 0.01 * dist_entropy

            # gradient step
            self.optimizer.zero_grad()
            loss.mean().backward()
            self.optimizer.step()

        # transfer weights
        self.policy_old.load_state_dict(self.policy.state_dict())


def main():
    env = gym.make('glider2D-v1')

    memory = Memory()
    ppo = PPO()

    _params_rl      = params_rl()
    _params_model   = params_model()
    _params_task    = params_task()
    _params_sim     = params_sim()
    _params_glider  = params_glider()
    _params_physics = params_physics()
    _params_logging = params_logging()

    returns = []
    average_returns = []
    evaluateGlider.main(env, ppo.policy, 0)

    for n_epi in range(int(_params_rl.N_EPOCH)):
        s = env.reset()
        s = env.standardize_observations(s)
        done = False
        ret = 0

        while not done:
            action = ppo.policy_old.act(torch.from_numpy(s).float(), memory)
            s_prime, r, done, info = env.step(action[0].numpy())

            memory.rewards.append(r)

            s = s_prime
            ret += r

            if done:
                returns.append(ret)
                break

        ppo.update(memory)
        memory.clear_memory()

        n_mean = _params_logging.PRINT_INTERVAL if len(returns) >= _params_logging.PRINT_INTERVAL else len(returns)
        average_returns.append(np.convolve(returns[-n_mean:], np.ones((n_mean,)) / n_mean, mode='valid')[0])

        if n_epi % _params_logging.PRINT_INTERVAL == 0 and n_epi != 0:
            print("# episode: {}, avg return over last {} episodes: {:.1f}".format(n_epi,
                                                                                   _params_logging.PRINT_INTERVAL,
                                                                                   average_returns[-1]))

        if n_epi % _params_logging.SAVE_INTERVAL == 0 and n_epi != 0:
            torch.save(ppo.policy.state_dict(), "actor_critic_network_episode_{}".format(n_epi) + ".pt")
            evaluateGlider.main(env, ppo.policy, n_epi)

    now = datetime.datetime.now()
    torch.save(ppo.policy.state_dict(), "actor_critic_network_final_" + now.strftime("%d-%B-%Y_%H-%M") + ".pt")
    evaluateGlider.main(env, ppo.policy, "final", True)
    f = open("parameters_" + now.strftime("%d-%B-%Y_%H-%M") + ".txt", "a+")
    f.write(format(vars(_params_rl)) + "\n" + format(vars(_params_model)) + "\n" + format(vars(_params_task)) + "\n" +
            format(vars(_params_sim)) + "\n" + format(vars(_params_glider)) + "\n" +
            format(vars(_params_physics)) + "\n" + format(vars(_params_logging)))
    f.close()

    plt.figure("average returns")
    plt.plot(average_returns)
    plt.ylabel("average returns (-)")
    plt.xlabel("episodes (-)")
    plt.grid(True)
    plt.savefig("average_returns_" + now.strftime("%d-%B-%Y_%H-%M") + ".png")

    env.close()

if __name__ == '__main__':
    main()