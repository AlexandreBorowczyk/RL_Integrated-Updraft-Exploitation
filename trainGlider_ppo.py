import gym
import glider
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import normal, MultivariateNormal
import numpy as np
import datetime
import matplotlib.pyplot as plt

from params import *
import evaluateGlider

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


class Memory:
    def __init__(self):
        self.actions = []
        self.states = []
        self.logprobs = []
        self.rewards = []

    def clear_memory(self):
        del self.actions[:]
        del self.states[:]
        del self.logprobs[:]
        del self.rewards[:]


class ActorCritic(nn.Module):
    def __init__(self):
        super(ActorCritic, self).__init__()

        self._params_model = params_model()
        self._params_rl = params_rl()

        self.hiddenLayer_in = nn.Linear(self._params_model.DIM_IN, self._params_model.DIM_HIDDEN)
        self.hiddenLayer_internal = nn.Linear(self._params_model.DIM_HIDDEN, self._params_model.DIM_HIDDEN)
        self.outputLayer_actor = nn.Linear(self._params_model.DIM_HIDDEN, self._params_model.DIM_OUT)
        self.outputLayer_critic = nn.Linear(self._params_model.DIM_HIDDEN, 1)

        self.action_var = torch.full((self._params_model.DIM_OUT,),
                                     self._params_rl.SIGMA * self._params_rl.SIGMA).to(device)

    def actor(self, x):
        x = torch.tanh(self.hiddenLayer_in(x))
        for ii in range(self._params_model.NUM_HIDDEN - 1):
            x = torch.tanh(self.hiddenLayer_internal(x))
        pi = torch.tanh(self.outputLayer_actor(x))
        return pi

    def critic(self, x):
        x = torch.tanh(self.hiddenLayer_in(x))
        for ii in range(self._params_model.NUM_HIDDEN - 1):
            x = torch.tanh(self.hiddenLayer_internal(x))
        v = self.outputLayer_critic(x)
        return v

    def act(self, state, memory):
        action_mean = self.actor(state)
        dist = MultivariateNormal(action_mean, torch.diag(self.action_var).to(device))
        action = dist.sample()
        action_logprob = dist.log_prob(action)

        memory.states.append(state)
        memory.actions.append(action)
        memory.logprobs.append(action_logprob)

        return action.detach()

    def evaluate(self, state, action):
        action_mean = []
        state_value = []
        no_states = len(state)
        for idx in range(no_states):  # stacked experience for multiple agents
            action_mean.append(self.actor(state[idx]))
            state_value.append(self.critic(state[idx]))
        action_mean = torch.stack(action_mean).view(-1, 1, self._params_model.DIM_OUT)
        state_value = torch.stack(state_value).view(-1, 1, 1)

        dist = MultivariateNormal(torch.squeeze(action_mean,1), torch.diag(self.action_var))
        action_logprobs = dist.log_prob(action)
        dist_entropy = dist.entropy()

        return action_logprobs, torch.squeeze(state_value), dist_entropy

class PPO:
    def __init__(self):
        self._params_model = params_model()
        self._params_rl = params_rl()

        self.policy = ActorCritic().to(device)

        ######Use the three commented lines if training should be continued from a certain training state#########
        # filename = "PPO_Continuous_drones2d-v0_288000_1562915157"
        # self.policy.load_state_dict(torch.load("./models/"+filename+".pth"))

        self.optimizer = torch.optim.Adam(self.policy.parameters(),
                                          lr=self._params_rl.LEARNING_RATE, betas=(0.9, 0.999))
        self.policy_old = ActorCritic().to(device)
        # self.policy_old.load_state_dict(torch.load("./models/"+filename+".pth"))
        self.MseLoss = nn.MSELoss()

    def select_action(self, state, memory):
        state = torch.FloatTensor(state.reshape(1, -1)).to(device)
        return self.policy_old.act(state, memory).cpu().data.numpy().flatten()

    def update(self, memory):
        # Estimation of rewrads
        rewards = []  # TODO: rename that variable -> returns
        discounted_reward = 0
        for reward in reversed(memory.rewards):
            discounted_reward = reward + (self._params_rl.GAMMA * discounted_reward)
            rewards.insert(0, discounted_reward)

        # normalize
        rewards = torch.tensor(rewards).to(device)
        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)

        # convert list to tensor
        old_states = memory.states
        old_actions = torch.stack(memory.actions).to(device).detach()
        old_logprobs = torch.squeeze(torch.stack(memory.logprobs)).to(device).detach()

        #####Policy Optimization for k Epochs######
        for _ in range(self._params_rl.K_EPOCH):
            # evaluation of old states and actions
            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)

            # ppo ratio
            ratios = torch.exp(logprobs - old_logprobs.detach())

            # surrogare loss
            advantages = rewards - state_values.detach()
            surr1 = ratios * advantages
            surr2 = torch.clamp(ratios, 1 - self._params_rl.EPS_CLIP, 1 + self._params_rl.EPS_CLIP) * advantages
            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, rewards) - 0.01 * dist_entropy

            # gradient step
            self.optimizer.zero_grad()
            loss.mean().backward()
            self.optimizer.step()

        # transfer weights
        self.policy_old.load_state_dict(self.policy.state_dict())


def main():
    env = gym.make('glider-v0')

    memory = Memory()
    ppo = PPO()

    _params_rl = params_rl()
    _params_logging = params_logging()

    returns = []
    average_returns = []

    for n_epi in range(int(_params_rl.N_EPOCH)):
        s = env.reset()
        s = env.standardize_observations(s)
        done = False
        ret = 0

        while not done:
            action = ppo.policy_old.act(torch.from_numpy(s).float(), memory)
            s_prime, r, done, info = env.step(action[0].numpy())

            memory.rewards.append(r)

            s = s_prime
            ret += r

            if done:
                returns.append(ret)
                break

        ppo.update(memory)
        memory.clear_memory()

        n_mean = _params_logging.PRINT_INTERVAL if len(returns) >= _params_logging.PRINT_INTERVAL else len(returns)
        average_returns.append(np.convolve(returns[-n_mean:], np.ones((n_mean,)) / n_mean, mode='valid')[0])

        if n_epi % _params_logging.PRINT_INTERVAL == 0 and n_epi != 0:
            print("# episode: {}, avg return over last {} episodes: {:.1f}".format(n_epi,
                                                                                   _params_logging.PRINT_INTERVAL,
                                                                                   average_returns[-1]))

        if n_epi % _params_logging.SAVE_INTERVAL == 0 and n_epi != 0:
            torch.save(ppo.policy.state_dict(), "actor_critic_network_episode_{}".format(n_epi) + ".pt")

    now = datetime.datetime.now()
    torch.save(ppo.policy.state_dict(), "actor_critic_network_final_" + now.strftime("%d-%B-%Y_%H-%M") + ".pt")

    plt.figure("average returns")
    plt.plot(average_returns)
    plt.ylabel("average returns (-)")
    plt.xlabel("episodes (-)")
    plt.grid(True)
    plt.savefig("average_returns" + now.strftime("%d-%B-%Y_%H-%M") + ".png")

    evaluateGlider.main(ppo.policy)
    env.close()


if __name__ == '__main__':
    main()