import torch
from torch.utils.data import Dataset, TensorDataset

from parameters import params_updraft_finder
from utils.LSTM_model import ActorCritic
from utils.PPO_data_handling import PPOBuffer, SlidingWindowDataset

class MyDataset(Dataset):
    def __init__(self, data, window):
        self.data = data
        self.window = window

    def __getitem__(self, index):
        x = self.data[index:index + self.window]
        return x

    def __len__(self):
        return len(self.data) - self.window

class PPO:
    def __init__(self, device=torch.device("cpu")):
        # instantiate parameters
        self._params_rl = params_updraft_finder.LearningParameters()
        self._params_model = params_updraft_finder.ModelParameters()

        # instantiate buffer for rollout
        self.buffer = PPOBuffer(self._params_model.DIM_IN, self._params_model.DIM_OUT, self._params_rl.BATCHSIZE,
                                self._params_model.DIM_LSTM, gamma=self._params_rl.GAMMA, lam=self._params_rl.LAMBDA)

        # instantiate actor-critic model
        self.model = ActorCritic(self._params_model, self._params_rl, device).to(device)

        # setup optimizers
        self.pi_optimizer = torch.optim.Adam(self.model.actor.parameters(), lr=self._params_rl.LEARNING_RATE_PI)
        self.vf_optimizer = torch.optim.Adam(self.model.critic.parameters(), lr=self._params_rl.LEARNING_RATE_VF)

    def select_action(self, state, lstm_hidden, validation_mask=False):
        # evaluate policy net
        action, action_logprob, lstm_hidden = self.model.act(state, lstm_hidden, validation_mask=validation_mask)

        # evaluate critic
        state_value = self.model.critic(state)

        return action, action_logprob, state_value, lstm_hidden

    def update(self):

        # get sampled data
        data = self.buffer.get()
        obs, act, ret, adv, logp, lstm_h_in, lstm_c_in, done \
            = data['obs'], data['act'], data['ret'], data['adv'], data['logp'],\
              data['lstm_h_in'], data['lstm_c_in'], data['done']

        # put batch to sliding-window data_loader
        data_set = MyDataset(TensorDataset(obs, act, ret, adv, logp, lstm_h_in, lstm_c_in, done),
                             self._params_rl.SEQ_LEN)

        # Optimize policy for K epochs:
        for _ in range(self._params_rl.K_EPOCH):
            for start_index in range(0, (data_set.__len__()) + 1,
                                     int(self._params_rl.SEQ_LEN - self._params_rl.OVERLAP)):
                # get sampled sequence/mini-batch
                obs_seq, act_seq, ret_seq, adv_seq, logp_seq, lstm_h_in_seq, lstm_c_in_seq, done_seq = \
                    data_set.__getitem__(start_index)

                if any(done_seq == 1):
                    #  never evaluate sequences that cross episode boundaries
                    done_index = done_seq.nonzero()[0].item()  # index of first done flag in sequence
                    if done_index > (self._params_rl.SEQ_LEN_MIN - 1):
                        obs_seq         = obs_seq[slice(0, done_index + 1)]
                        act_seq         = act_seq[slice(0, done_index + 1)]
                        ret_seq         = ret_seq[slice(0, done_index + 1)]
                        adv_seq         = adv_seq[slice(0, done_index + 1)]
                        logp_seq        = logp_seq[slice(0, done_index + 1)]
                        lstm_h_in_seq   = lstm_h_in_seq[slice(0, done_index + 1)]
                        lstm_c_in_seq   = lstm_c_in_seq[slice(0, done_index + 1)]
                    else:
                        break

                # "burn in" lstm hidden state (cf. "R2D2")
                with torch.no_grad():
                    _, (lstm_h_burned_in, lstm_c_burned_in) =\
                        self.model.actor(obs_seq[0:self._params_rl.N_BURNIN, :], (lstm_h_in_seq[0], lstm_c_in_seq[0]))

                # evaluate policy for remainder sampled sequence of states and actions
                logp_eval = self.model.evaluate_actor(obs_seq[self._params_rl.N_BURNIN:, :],
                                                      act_seq[self._params_rl.N_BURNIN:],
                                                      (lstm_h_burned_in, lstm_c_burned_in))

                # ppo ratio
                ratios = torch.exp(logp_eval - logp_seq[self._params_rl.N_BURNIN:])

                # surrogate loss (PPO)
                surr1 = ratios * adv_seq[self._params_rl.N_BURNIN:]
                surr2 = torch.clamp(ratios, 1 - self._params_rl.EPS_CLIP, 1 + self._params_rl.EPS_CLIP)\
                        * adv_seq[self._params_rl.N_BURNIN:]
                loss_pi = -torch.min(surr1, surr2).mean()

                # policy gradient step
                self.pi_optimizer.zero_grad()
                loss_pi.backward()
                nn.utils.clip_grad_norm_(self.model.actor.parameters(), 1.0)
                self.pi_optimizer.step()

                # value function gradient step
                loss_vf = ((self.model.critic(obs_seq) - ret_seq)**2).mean()
                self.vf_optimizer.zero_grad()
                loss_vf.backward()
                nn.utils.clip_grad_norm_(self.model.critic.parameters(), 1.0)
                self.vf_optimizer.step()